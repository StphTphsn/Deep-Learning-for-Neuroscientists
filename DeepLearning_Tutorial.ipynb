{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Tutorial on deep learning using Keras and Tensorflow\n",
    "\n",
    "Tensorflow (2015) and Keras (2015) are gradient-based optimization libraries (e.g. deep learning libraries) developped by Google.\n",
    "\n",
    "Why and when use Tensorflow?\n",
    "\n",
    "- Tensorflow allows you to automatically find the gradient of complex functions (such as multi-layered or recurrent neural networks) and performs gradient-based optimization with state-of-the-art optimizers (e.g. stochastic gradient descent, ADAM optmizer, etc.)\n",
    "- Seamlessly ports the computations on one or several GPUs (up to 50x faster than CPU)\n",
    "- Ideal for training deep networks (or even shallow networks) on small or large datasets.\n",
    "\n",
    "Why and when use Keras?\n",
    "\n",
    "- Keras is a high-level library (coarser 'bricks') running on Tensorflow (or Theano).\n",
    "- It simplifies the code (a lot) when creating classical networks\n",
    "- When using Keras, one can always customize the network more finely by returning to Tensorflow\n",
    "- Use Keras whenever you would use Tensorflow, and return to Tensorflow to customize the network\n",
    "\n",
    "Alternative deep learning libraries: Among many others, PyTorch (developped by Facebook), Theano (MILA group, will be discontinued!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "Multilayer perceptron (MLP) is the most basic form of artificial neural networks. Inputs are transformed into outputs through multiple layers of feedfoward processing.\n",
    "\n",
    "For an input vector $\\mathbf{x}$, a simple 1-hidden-layer MLP will convert the input to an output vector $\\mathbf{y}$, through a hidden layer of neurons $\\mathbf{h}$. For simplicity, we will ignore the bias terms.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{h} &=& f(W_h\\mathbf{x}) \\\\\n",
    "\\mathbf{y} &=& W_y\\mathbf{h}\n",
    "\\end{eqnarray}\n",
    "\n",
    "$W_h, W_y$ are the weight matrices, and $f(\\cdot)$ is the activation function for the hidden layer. The trainable parameters will be collectively referred to as the parameters $\\theta$.\n",
    "\n",
    "Our goal is to find parameters ($W_h, W_y$) that allow the predicted output to be close to the target output $\\hat{y}$. We define the loss as\n",
    "\\begin{equation}\n",
    "L = \\langle(\\mathbf{y}-\\hat{y})^2\\rangle\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## Stochastic gradient descent\n",
    "We will use (variants of) stochastic gradient descent to decrease the loss. At each step, we will evaluate the loss using a minibatch of inputs.\n",
    "\\begin{equation}\n",
    "\\tilde{L} = (\\mathbf{y}_i-\\hat{y}_i)^2, \\ \\mathrm{for \\ i = 1, ..., batch size}\n",
    "\\end{equation}\n",
    "\n",
    "We move the parameters in the opposite direction of the gradient:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta \\theta =-\\alpha \\frac{\\partial \\tilde{L}}{\\partial \\theta},\n",
    "\\end{equation}\n",
    "$\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Multilayer perceptron in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Numpy way\n",
    "batch_size = 5\n",
    "n_x = 10\n",
    "n_h = 100\n",
    "n_y = 3\n",
    "x_np = np.random.rand(batch_size, n_x)\n",
    "def relu(x):\n",
    "    return x * (x>0.)\n",
    "\n",
    "W_h_np = np.random.randn(n_x, n_h) * np.sqrt(2./(n_x + n_h))\n",
    "W_y_np = np.random.randn(n_h, n_y) * np.sqrt(2./(n_h + n_y))\n",
    "\n",
    "h_np = relu(np.dot(x_np, W_h_np))\n",
    "y_np = np.dot(h_np, W_y_np)\n",
    "\n",
    "y_target_np = np.random.rand(batch_size, n_y)\n",
    "loss_np = np.mean((y_target_np - y_np)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Multilayer perceptron in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow way\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, (batch_size, n_x))\n",
    "\n",
    "# Define variables\n",
    "W_h = tf.get_variable('W_h', shape=(n_x, n_h), initializer=tf.constant_initializer(W_h_np))\n",
    "W_y = tf.get_variable('W_y', shape=(n_h, n_y), initializer=tf.constant_initializer(W_y_np))\n",
    "\n",
    "# Build the computational graph\n",
    "h = tf.nn.relu(tf.matmul(x, W_h))\n",
    "y = tf.matmul(h, W_y)\n",
    "\n",
    "print(y)\n",
    "\n",
    "sess = tf.Session()\n",
    "# with tf.Session() as sess:\n",
    "sess.run(tf.global_variables_initializer())\n",
    "y_val = sess.run(y, feed_dict={x: x_np})\n",
    "sess.close()\n",
    "# This should be (batch_size, n_y)\n",
    "print(y_val)    \n",
    "\n",
    "plt.scatter(y_val, y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Multilayer perceptron in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Keras way\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_h, input_dim=n_x, activation='relu', use_bias=False))\n",
    "model.add(Dense(n_y, activation=None, use_bias=False))\n",
    "model.layers[0].set_weights([W_h_np])\n",
    "model.layers[1].set_weights([W_y_np])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Q0: Compare the two networks outputs\n",
    "The NumPy network and the Keras network are identical. Verify it by feeding the NumPy network and the Keras network with 10 random gaussian inputs. Compare the response of the two networks to these inputs. Are they the same?\n",
    "\n",
    "Help: to obtain the output Y of the Keras network to an input X, use the function\n",
    "Y = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Training a Keras network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Load MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "for i in range(3):\n",
    "    plt.figure(figsize = (2,2))\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create network and train on MNIST\n",
    "\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size = 32)\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Q1: Observation questions\n",
    "1.1 How many layers of neurons are there in this network? <br/>\n",
    "1.2 How many neurons are there in each layer? <br/>\n",
    "1.3 What is the non-linearity used in the first layer? <br/>\n",
    "1.4 What is the non-linearity used in the second layer? What is the softmax function? <br/>\n",
    "1.5 Why do we use the layer 'Flatten'? <br/>\n",
    "1.6 What is the optimization method used? <br/>\n",
    "1.7 What loss function are we using? <br/>\n",
    "1.8 What batch size are we using? <br/>\n",
    "1.9 What is the accuracy of the trained network on the training set? And on the testing set? Is the network overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Q2: Play with the network hyperparameters\n",
    "\n",
    "Copy the original network in different cells of the notebook and perform the following changes.\n",
    "\n",
    "2.1 Set the number of neurons in the first layer to 10. How does this affect training accuracy? Testing accuracy? Is the network overfitting?<br/>\n",
    "\n",
    "2.2 Add a second layer of neurons identical to the first layer. How does it affect performance?<br/>\n",
    "\n",
    "2.3 Remove the non-linearity of the first layer. How does it affect performance?\n",
    "\n",
    "2.4 (optional) What is a 'Dropout' layer? Is it active doing training only or also during testing? Add a dropout layer after the first layer. How does it affect the training and testing accuracies?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Q3: Visualize the inner workings of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 3.1 Neuron activations in the hidden layer\n",
    "Select two neurons of the first layer (i.e. hidden layer) and plot their activations across the testing set in a scatterplot (x-axis : activation of neuron1, y-axis : activation of neuron2). Are these neurons correlated?<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# CODE HELP\n",
    "#get output of a layer 'layer_nb' for an input (here 600 samples of a random gaussian input):\n",
    "from keras import backend as K\n",
    "layer_nb = 1\n",
    "inp = model.input # input placeholder\n",
    "out = model.layers[layer_nb].output # layer output\n",
    "func = K.function([inp], [out]) # function relating input to output\n",
    "output = func([np.random.rand(600,28,28)])[0]  #  apply function to a particular input\n",
    "print('Output shape:', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###  3.2 Visualize hidden layer representation in Principal Component space\n",
    "\n",
    "a) Perform SVD on the activation matrix of the hidden layer (dim1: MNIST samples, dim2: neuron responses). Represent each MNIST sample in a scatterplot where the x-axis is the projection of the sample representation along PC1 and the y-axis the projection along PC2. Color the points by the corresponding class of the sample (0 to 9). Are the classes well separated in PC space? <br/>\n",
    "\n",
    "b) Re-do this analysis before training the network. Are the classes better separated after training?  \n",
    "\n",
    "c) Re-do this analysis (a and b) in T-SNE space instead of PC space, before and after training. <br/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# CODE HELP\n",
    "# do SVD on a matrix (e.g on matrix 'output')\n",
    "U,S,V = np.linalg.svd(output)\n",
    "print('U shape:', U.shape)\n",
    "print('S shape:', S.shape)\n",
    "print('V shape:', V.shape)\n",
    "\n",
    "# scatterplot\n",
    "plt.figure(figsize = (5,5))\n",
    "for i in range(10):  \n",
    "    targets = y_test[0:600]==i\n",
    "    plt.plot(U[targets,0],U[targets,1],'.')\n",
    "\n",
    "# do TSNE on a matrix\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=30.0)\n",
    "R = tsne.fit_transform(output)\n",
    "R.shape\n",
    "\n",
    "# scatterplot\n",
    "plt.figure(figsize = (5,5))\n",
    "for i in range(10):  \n",
    "    targets = y_test[0:600]==i\n",
    "    plt.plot(R[targets,0],R[targets,1],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###  3.3 Receptive fields of the hidden layer\n",
    "a) Select a few neurons of the first layer and represent their receptive field (i.e. weights on the input image). <br/>\n",
    "\n",
    "b) Create a white noise stimulus database with 10,000 samples. Feed it to the network. Perform activation-weighed average (i.e. STA) for the neurons selected in (a). Do you recover the neurons receptive fields? <br/>\n",
    "\n",
    "BONUS QUESTION: Find the RF of the neurons of the second layer by performing STA. Alternatively, find the RF of these neurons by doing a one step gradient descent on the activation of this neuron from a gray image. Why are these two computations giving the same result? <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# get weights and biases of a layer 'layer_nb'\n",
    "W,b = model.layers[layer_nb].get_weights()\n",
    "print('Weights shape:', W.shape, \"biases shape\", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[1].output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### BONUS QUESTION: \n",
    "Find the RF of the neurons of the second layer by performing STA. Alternatively, find the RF of these neurons by doing a one step gradient descent on the activation of this neuron from a gray image. Why are these two computations giving the same result? <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# NO HELP!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Other Ressource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Installing Tensorflow and Keras (CPU-only, already installed on the lab computers)\n",
    "1) Download and intall Anaconda for Python 3.6 https://www.anaconda.com/download/#macos <br/>\n",
    "2) Install Tensorflow from command line: conda install -c conda-forge tensorflow <br/>\n",
    "3) Install Keras from command line: conda install -c conda-forge keras <br/>\n",
    "Need access to a FREE GPU in a few clicks? Check out Colab: https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Training the network with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow way\n",
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()\n",
    "\n",
    "x = tf.placeholder(tf.float32, (batch_size, n_x))\n",
    "\n",
    "# Define variables\n",
    "W_h = tf.get_variable('W_h', shape=(n_x, n_h), initializer=tf.constant_initializer(W_h_np))\n",
    "W_y = tf.get_variable('W_y', shape=(n_h, n_y), initializer=tf.constant_initializer(W_y_np))\n",
    "\n",
    "# Build the computational graph\n",
    "h = tf.nn.relu(tf.matmul(x, W_h))\n",
    "y = tf.matmul(h, W_y)\n",
    "\n",
    "print(y)\n",
    "\n",
    "sess = tf.Session()\n",
    "# with tf.Session() as sess:\n",
    "sess.run(tf.global_variables_initializer())\n",
    "y_val = sess.run(y, feed_dict={x: x_np})\n",
    "sess.close()\n",
    "# This should be (batch_size, n_y)\n",
    "print(y_val)    \n",
    "\n",
    "plt.scatter(y_val, y_np)\n",
    "\n",
    "\n",
    "# Defining the target output and the loss\n",
    "y_target = tf.placeholder(tf.float32, (None, n_y))\n",
    "loss = tf.reduce_mean((y_target - y)**2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss_val = sess.run(loss, feed_dict={x: x_np, y_target: y_target_np})\n",
    "    \n",
    "print(loss_np)\n",
    "print(loss_val)\n",
    "\n",
    "# Compute the gradient and train\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = opt.minimize(loss)\n",
    "# opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# We will overfit the network on the random target output we generated\n",
    "steps = list()\n",
    "losses = list()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10):\n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                               feed_dict={x: x_np, y_target: y_target_np})\n",
    "        if step % 1 == 0:\n",
    "            print(step, loss_val)\n",
    "            steps.append(step)\n",
    "            losses.append(loss_val)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, input_shape = (28,28,1), kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train[..., np.newaxis], y_train, epochs=5)\n",
    "model.evaluate(x_test[..., np.newaxis], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Recurrent network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, GRU\n",
    "from keras import Input\n",
    "\n",
    "x_train_rnn = x_train.reshape((x_train.shape[0], -1, 1))\n",
    "x_test_rnn = x_train.reshape((x_test.shape[0], -1, 1))\n",
    "\n",
    "x = Input((None, x_train_rnn.shape[-1]))\n",
    "# layer = tf.keras.layers.SimpleRNN(100)\n",
    "# layer = tf.keras.layers.SimpleRNN(100)\n",
    "layer = GRU(20)\n",
    "y = layer(x)\n",
    "print(x)\n",
    "model = keras.models.Model(inputs=x, outputs=y)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train_rnn, y_train, epochs=5)\n",
    "model.evaluate(x_test_rnn, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# the loss is here the output of a neuron\n",
    "loss = model.layers[2].output[:,4]\n",
    "input_img = model.input\n",
    "\n",
    "# we compute the gradient of the input picture wrt to this loss\n",
    "grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "# this function returns the loss and grads given the input picture\n",
    "iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "# we start from a gray image\n",
    "input_img_data = np.zeros((1,28,28))\n",
    "\n",
    "# we run gradient ascent for 1 step so it's just a computation of the gradient\n",
    "loss_value, grads_value = iterate([input_img_data])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(grads_value[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
